{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/NCAI-Research/CALM/blob/volunteers-instructions/assets/logo.png?raw=true\" atl=\"CALM Logo\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "Mxw_A5xAPQzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was written for the Collaborative Arabic Language Model CALM project, it will contain instructions on how to set up your collaborative training.\n",
        "\n",
        "\n",
        "* For more information, please visit https://github.com/NCAI-Research/CALM and https://huggingface.co/CALM. \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MA5Yku4K6P-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ“£ Pre-training required steps: \n",
        "1.   Create a [**Huggingface account**](https://huggingface.co) and join the NCAI-CALM organization ðŸ‘‰ðŸ» https://huggingface.co/CALM using the link sent to you on the invitation email.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LOpeVP43kwvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Clone the repo"
      ],
      "metadata": {
        "id": "V_TFg2G8JfwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NCAI-Research/CALM"
      ],
      "metadata": {
        "id": "SMtzG60nJe-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Installing required libraries\n",
        "\n",
        "NOTE: be patient this may take a couple of minutes."
      ],
      "metadata": {
        "id": "oS8HeWuM9rCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing requirements...\")\n",
        "%cd CALM\n",
        "!pip install -q -r requirements.txt &> log"
      ],
      "metadata": {
        "id": "bQ727XTz9mWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Setup the experiment environment variables\n"
      ],
      "metadata": {
        "id": "ATEZ9-jM-U2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the name of the experiment\n",
        "exp_name = \"CALM\"\n",
        "\n",
        "# the name of the HF organization and model for the experiment\n",
        "%env HF_ORGANIZATION_NAME=CALM\n",
        "%env HF_MODEL_NAME={exp_name}\n",
        "\n",
        "# WANDB information for tracking the run \n",
        "\n",
        "%env WANDB_API_KEY=65dbae2761bd93ee41c54b443c361114be29b8ec\n",
        "\n",
        "# Name, project, and method for the WANDB Team\n",
        "%env WANDB_ENTITY=calm\n",
        "%env WANDB_PROJECT={exp_name}-hivemind-trainers\n",
        "%env WANDB_START_METHOD=thread"
      ],
      "metadata": {
        "id": "w1d4ZenoXmPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the user authority in the HF organization ðŸ¤—\n",
        "\n",
        "When the code runs it will request for the user access token ðŸ”‘ in HF, to get it:\n",
        "\n",
        "1. Go to your [HF account](https://huggingface.co)\n",
        "2. Go to Settings â‡’ Access Tokens\n",
        "3. Generate a new Access Token and enter any name for \"what's this token for\"\n",
        "4. Select `read` role\n",
        "5. Copy your access token\n",
        "6. Paste it in the execution prompt in the notebook\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KBtHUlmQl0n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_auth import authorize_with_huggingface\n",
        "\n",
        "os.environ['HF_USER_ACCESS_TOKEN'] = authorize_with_huggingface().hf_user_access_token"
      ],
      "metadata": {
        "id": "bB8NCsPLMNDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the punkt sentence tokenizer\n"
      ],
      "metadata": {
        "id": "ct2wNwSeLQqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "0nYt0Yi5TWuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Let's start training ðŸ‘ ðŸ•–\n",
        " "
      ],
      "metadata": {
        "id": "NS8hkL6-kwQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the device capability to set the batch size\n",
        "import torch\n",
        "capability = torch.cuda.get_device_capability()\n",
        "memory_gb = torch.cuda.mem_get_info()[1] / 1e9\n",
        "gradient_checkpointing = False\n",
        "if capability >= (8, 0):  # ampere\n",
        "  batch_size, fp16 = 8, True\n",
        "elif capability >= (6, 0):  # v100, t4, p100\n",
        "  batch_size, fp16 = 4, True\n",
        "else:  # maxwell, kepler\n",
        "  batch_size, fp16 = 1, False\n",
        "if memory_gb < 9:  # 8gb gpus: 1070, 2060S, \n",
        "  batch_size = min(batch_size, 2)\n",
        "if memory_gb < 7:  # 6gb or less: try our best to fit\n",
        "  batch_size, fp16 = min(batch_size, 1), True\n",
        "  gradient_checkpointing = True\n",
        "print(f\"\\nRunning {torch.cuda.get_device_name()}, setting batch size = {batch_size}, fp16 = {fp16}, gradient_checkpointing={gradient_checkpointing}\\n\")\n",
        "\n",
        "# start the training\n",
        "!ulimit -n 16384 && python run_trainer.py --run_id {exp_name} --per_device_train_batch_size {batch_size} --gradient_accumulation_steps 1 --fp16 {fp16} --gradient_checkpointing {gradient_checkpointing} \\\n",
        "  --client_mode --matchmaking_time 60 --initial_peers /ip4/34.124.232.172/tcp/12345/p2p/QmdGDSzDEi7uo8pTGG7n8s2dW12VGoPQKiDVDoQaVAo3bf /ip4/193.106.95.184/tcp/12345/p2p/QmRgdEXySu8hEB3xUxexJPxcv7M41PggRDnUTf9kStdgup /ip4/194.213.3.15/tcp/12345/p2p/QmYSF8GSLWxjJxSrtpAbdQGRSxbDT81MruruEcxNaDcZCD"
      ],
      "metadata": {
        "id": "n1vNHy9vE93Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}